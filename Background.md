## Federated EM Algorithm

Federated learning is motivated by privacy concerns as it does not involve transmitting private data but only intermediate updates. It allows training a global model over multiple nodes without sharing each node's local data directly. Each node trains a local model, and these local models are combined into a global model. In our learning mission, we aim to learn the parameters of data generated by a Gaussian mixture model, a probabilistic model assuming data points are generated from a mixture of Gaussian distributions with unknown parameters.

### Fundamentals of GMM and EM over Networks

We introduce the basics of the Gaussian Mixture Model (GMM). Suppose we have 'n' clients, each with its own 'x_i' data, where 'i' is in the range of {1, 2, ..., n}. The GMM density is given by:

p(x) = ‚àë Œ≤_jp(x|Œº_j , Œ£_j )

Where:
- 'p(x|Œº_j, Œ£_j)' is the PDF function for the Gaussian distribution.
- 'Œ£_j' is the covariance matrix.
- 'Œº_j' is the covariance.
- The mixture coefficient is 'Œ≤_j' for 'j' in 'C', where 'C' is the number of Gaussian models (clusters).

The goal is to estimate 'Œ£_j', 'Œº_j', and 'Œ≤_j' for each 'j' in 'C'. To do this over a network, we should first understand how to model a network. Networks are generally modeled as undirected graphs 'G = {V, E}' where 'E ‚äÜ V √ó V', and 'V = {1, 2, ..., n}' nodes. Each node can communicate only through an edge connecting it to another node.

To estimate the latent variables through the EM algorithm with available data at each node, we follow these steps for iteration 't':

**E-step:**

P(x_i|N^t_j) = (p(x_i|Œº_j, Œ£_j)Œ≤^t_j) / Œ£^c_{k=1} (p(x_i|Œº_k, Œ£_k)Œ≤^t_k)


**M-step:**

Œ≤^{t+1}j = (Œ£^{n}{i=1} P(x_i|N_{j}^t)) / n
Œº^{t+1} = (Œ£^{n}{i=1} P(x_i|N{j}^{t}) x_i) / (Œ£^{n}{i=1} P(x_i|N{j}^t))
Œ£_{j}^{t+1} = (Œ£^{n}{i=1} P(x_i|N{j}^{t}) (x_i - Œº_{j}^{t})(x_i - Œº_{j}^{t})^T) / (Œ£^{n}{i=1} P(x_i|N{j}^t))

### Private Data and Adversary Models

Private data refers to individual data held by each client (i.e., node) 'i', often containing sensitive information, such as a person's health condition indicator like blood pressure. Therefore, each client (node) 'i' wants to prevent the exposure of its private data 'x_i' to other nodes (clients) and the server during computation.

We define the adversary model to protect against as the passive adversary model:

This model controls a number of corrupted clients who follow the protocol but can collect information together. It uses the collected information to infer the private data of the non-corrupted clients, known as honest clients.

### Federated EM Algorithm for GMM

The goal of federated learning is to learn the model under the constraint that data is stored and processed locally. Intermediate updates are transmitted at regular intervals to a central server. (See references [1][2]).

Within the scope of the EM algorithm, instead of directly transmitting the data 'x_i', each client 'i' shares the following intermediate updates only:

a_{i j}^{t} = P(x_i, N_{j}^{t})
b_{i j}^{t} = P(x_i, N_{j}^{t}) * x_i
c_{i j}^{t} = P(x_i, N_{j}^{t}) * (x_i - Œº_j^t) * (x_i - Œº_j^t)


All these updates can also be computed locally at client 'i'. After receiving these intermediate updates from all clients, the server consolidates all updates from local sources and determines the global update 'Œ≤_j^{t+1}', 'Œº_j^{t+1}', 'Œ£{j}^{t+1}' necessary for the M-step using the following method:

Œ≤_j^{t+1} = Œ£_{i=1}^n {a}{ij}^t / n
Œº_j^{t+1} = Œ£{i=1}^n {b}{ij}^t / Œ£{i=1}^n {a}{ij}^t = {b}{j}^t / {a}{j}^t
Œ£{j}^{t+1} = Œ£{i=1}^n {c}{ij}^t / Œ£{i=1}^n {a}{ij}^t = {c}{j}^t / {a}_{j}^t


Following that, the server transmits the global update 'Œ≤_j^{t+1}', 'Œº_j^{t+1}', 'Œ£{j}^{t+1}' back to each and every client.

### Privacy Vulnerability in Federated EM Algorithm

Using the federated EM algorithm described above, despite not directly sharing private data at each node, the data is still disclosed to the server due to the intermediate updates 'a_{ij}^t', 'b_{ij}^t' enabling the server to infer the private data 'x_i' of each client 'i', because of the equation:

{b}{ij}^t = {a}{ij}^t * x_i

In other words, during each iteration, the server possesses the following mutual information:

I(ùëã_i;ùê¥^t_{i},ùêµ^t_{i}) = I(ùëã_i;ùëã_i)

which is maximal.
